name: High-Level Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  high-level-tests:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: [3.12]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y curl redis-server

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

          # Install gRPC tools for protocol buffer compilation
          pip install grpcio-tools pytest-timeout

      - name: Start Redis service
        run: |
          sudo systemctl start redis-server
          sudo systemctl enable redis-server
          
          # Wait for Redis to be ready
          timeout 30 bash -c 'until redis-cli ping; do sleep 1; done'

      - name: Install and setup Ollama
        run: |
          # Install Ollama
          curl -fsSL https://ollama.ai/install.sh | sh

          # Start Ollama service in background
          ollama serve &

          # Wait for Ollama to be ready
          timeout 60 bash -c 'until curl -f http://localhost:11434/api/tags; do sleep 2; done'

          # Pull TinyLlama model
          ollama pull tinyllama

          # Verify model is available
          ollama list

      - name: Install Playwright browsers
        run: |
          playwright install --with-deps

      - name: Generate Protocol Buffers for gRPC
        run: |
          # Create proto directory if it doesn't exist
          mkdir -p proto

          # Generate gRPC Python files if proto file exists
          if [ -f "proto/inference.proto" ]; then
            python -m grpc_tools.protoc --proto_path=. --python_out=. --grpc_python_out=. proto/inference.proto
          else
            echo "âš ï¸  No proto file found, skipping gRPC code generation"
          fi

      - name: Create models directory and train model
        run: |
          mkdir -p models
          python scripts/train_iris_model.py

          # Also create the improved model for HTTP server compatibility
          python scripts/train_iris_model_improved.py

      - name: Start Flask application
        run: |
          # Start Flask app in background
          python app.py &
          FLASK_PID=$!

          # Wait for Flask app to be ready
          timeout 60 bash -c 'until curl -f http://localhost:5001/health; do sleep 2; done'

          # Verify all endpoints are accessible
          curl -f http://localhost:5001/health

          # Log the process ID for potential debugging
          echo "Flask app started with PID: $FLASK_PID"
        env:
          FLASK_ENV: testing
          OLLAMA_HOST: http://localhost:11434
          GRPC_SERVER_HOST: localhost
          GRPC_SERVER_PORT: 50051

      - name: Start HTTP inference server for fair performance comparison
        run: |
          # Start HTTP inference server in background for network-based comparison
          echo "ðŸš€ Starting HTTP inference server on port 5002..."
          python http_server.py &
          HTTP_SERVER_PID=$!

          # Wait for HTTP server to be ready
          timeout 30 bash -c 'until curl -f http://localhost:5002/health; do sleep 2; done'

          # Verify HTTP inference server is responding
          curl -f http://localhost:5002/health

          # Log the process ID for potential debugging
          echo "HTTP inference server started with PID: $HTTP_SERVER_PID"

      - name: Start gRPC server (optional)
        run: |
          # Start gRPC server in background if proto files exist
          if [ -f "grpc_server.py" ] && [ -f "proto/inference_pb2.py" ]; then
            echo "ðŸš€ Starting gRPC server..."
            python grpc_server.py &
            
            # Wait a bit for gRPC server to start
            sleep 5
            
            # Test if gRPC server is responding (optional, non-blocking)
            python -c "import grpc; from proto import inference_pb2_grpc, inference_pb2; channel = grpc.insecure_channel('localhost:50051'); stub = inference_pb2_grpc.InferenceServiceStub(channel); print('gRPC server is running')" || echo "âš ï¸  gRPC server not responding, tests will run without it"
          else
            echo "âš ï¸  gRPC components not found, skipping gRPC server startup"
          fi

      - name: Verify all servers are running
        run: |
          echo "ðŸ” Checking server availability for Phase 2 testing..."

          # Check Flask app
          if curl -f http://localhost:5001/health > /dev/null 2>&1; then
            echo "âœ… Flask Server: Available at localhost:5001"
          else
            echo "âŒ Flask Server: Not available at localhost:5001"
          fi

          # Check HTTP inference server  
          if curl -f http://localhost:5002/health > /dev/null 2>&1; then
            echo "âœ… HTTP Inference Server: Available at localhost:5002"
          else
            echo "âŒ HTTP Inference Server: Not available at localhost:5002"
          fi

          # Check gRPC server
          if python -c "import grpc; from proto import inference_pb2_grpc, inference_pb2; channel = grpc.insecure_channel('localhost:50051'); stub = inference_pb2_grpc.InferenceServiceStub(channel)" > /dev/null 2>&1; then
            echo "âœ… gRPC Inference Server: Available at localhost:50051"
          else
            echo "âŒ gRPC Inference Server: Not available at localhost:50051"
          fi

          # Check Ollama
          if curl -f http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo "âœ… Ollama LLM Service: Available at localhost:11434"  
          else
            echo "âŒ Ollama LLM Service: Not available at localhost:11434"
          fi

          echo ""
          echo "ðŸ“Š Server Status Summary Complete - Tests will run with graceful degradation for unavailable services"

      - name: Run High-Level Integration Tests
        run: |
          # Test core API endpoints and features
          python -m pytest tests/test_e2e_playwright.py -v --tb=short --junit-xml=test-results-e2e.xml
        env:
          PYTEST_CURRENT_TEST: true
          FLASK_ENV: testing
          OLLAMA_HOST: http://localhost:11434
          GRPC_SERVER_HOST: localhost
          GRPC_SERVER_PORT: 50051

      - name: Run Phase 2 Feature Tests
        run: |
          # Test main Phase 2 features (chat, gRPC performance, security)
          python -m pytest tests/test_phase2_features.py -v --tb=short --junit-xml=test-results-phase2.xml
        env:
          PYTEST_CURRENT_TEST: true
          FLASK_ENV: testing
          OLLAMA_HOST: http://localhost:11434

      - name: Run Phase 3 Feature Tests
        run: |
          # Test Phase 3 features (Redis caching, semantic caching, API versioning)
          python -m pytest tests/test_phase3_features.py -v --tb=short --junit-xml=test-results-phase3.xml
        env:
          PYTEST_CURRENT_TEST: true
          FLASK_ENV: testing
          OLLAMA_HOST: http://localhost:11434
          REDIS_URL: redis://localhost:6379

      - name: Run Security and Performance Tests
        run: |
          # Test security features and performance comparison
          python -m pytest tests/ -v -m "security or performance" --tb=short --junit-xml=test-results-security.xml
        env:
          PYTEST_CURRENT_TEST: true
          FLASK_ENV: testing
          OLLAMA_HOST: http://localhost:11434
          GRPC_SERVER_HOST: localhost
          GRPC_SERVER_PORT: 50051

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: test-results-*.xml

      - name: Generate test summary
        if: always()
        run: |
          echo "## ðŸ§ª High-Level Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count test result files
          total_suites=$(ls test-results-*.xml 2>/dev/null | wc -l || echo "0")
          echo "ðŸ“Š **Total Test Suites**: $total_suites" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### ðŸŽ¯ High-Level Features Tested:" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… End-to-end API workflows" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Core Phase 2 features (chat, gRPC, security)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Core Phase 3 features (Redis caching, semantic caching, API versioning)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Performance comparison (REST vs gRPC)" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Security validation and input sanitization" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Service integration and availability" >> $GITHUB_STEP_SUMMARY

  manual-verification:
    runs-on: ubuntu-latest
    needs: high-level-tests
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install and setup Ollama
        run: |
          curl -fsSL https://ollama.ai/install.sh | sh
          ollama serve &
          timeout 60 bash -c 'until curl -f http://localhost:11434/api/tags; do sleep 2; done'
          ollama pull tinyllama

      - name: Create models and start application
        run: |
          mkdir -p models
          python scripts/train_iris_model.py
          python app.py &
          timeout 60 bash -c 'until curl -f http://localhost:5001/health; do sleep 2; done'

      - name: Verify main endpoints
        run: |
          # Test health endpoint
          curl -f http://localhost:5001/health
          
          # Test classification endpoint
          curl -f -X POST http://localhost:5001/api/v1/classify \
            -H "Content-Type: application/json" \
            -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
          
          # Test secure generation (will hit Ollama)
          curl -f -X POST http://localhost:5001/api/v1/generate-secure \
            -H "Content-Type: application/json" \
            -d '{"prompt": "What is AI?"}'

      - name: Test batch processing
        run: |
          python scripts/batch_inference.py --create-samples
          python scripts/batch_inference.py --input sample_prompts.txt --output test_results.json
          test -f test_results.json
